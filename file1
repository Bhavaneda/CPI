import pandas as pd
from datetime import datetime
from pymongo import MongoClient
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.vector_ar.var_model import VAR
from sklearn.metrics import mean_squared_error
import numpy as np
import os
import warnings

# Suppress warnings
warnings.filterwarnings("ignore")

# MongoDB connection setup
client = MongoClient('mongodb://localhost:27017/')
db = client['stockdata']
collection = db['daily_price']

def fetch_data_from_mongodb(ticker):
    cursor = collection.find({'Ticker': ticker})
    df = pd.DataFrame(list(cursor))
    return df

def preprocess_data(df):
    df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')
    df.set_index('Date', inplace=True)
    df.sort_index(inplace=True)
    df.index = pd.DatetimeIndex(df.index.values, freq=df.index.inferred_freq)
    return df[['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']]

def split_train_test(data, split_date):
    split_date = pd.to_datetime(split_date)
    train = data[data.index <= split_date]
    test = data[data.index > split_date]
    return train, test

def train_sarima_model(train_data, order=(1, 1, 1), seasonal_order=(1, 1, 1, 7)):
    sarima_model = SARIMAX(train_data['Adj Close'], order=order, seasonal_order=seasonal_order, enforce_stationarity=False)
    sarima_fit = sarima_model.fit(disp=False)
    return sarima_fit

def train_sarimax_model(train_data, order=(1, 1, 1), seasonal_order=(1, 1, 1, 7)):
    exog_vars = train_data[['Open', 'High', 'Low', 'Adj Close', 'Volume']]
    sarimax_model = SARIMAX(train_data['Adj Close'], exog=exog_vars, order=order, seasonal_order=seasonal_order, enforce_stationarity=False)
    sarimax_fit = sarimax_model.fit(disp=False)
    return sarimax_fit

def train_var_model(train_data, lag_order=8):
    var_model = VAR(train_data)
    var_fit = var_model.fit(lag_order)
    return var_fit

def forecast_sarima_model(model, test_data):
    forecast = model.forecast(steps=len(test_data))
    return forecast

def forecast_sarimax_model(model, test_data):
    exog_vars = test_data[['Open', 'High', 'Low', 'Adj Close', 'Volume']]
    forecast = model.get_forecast(steps=len(test_data), exog=exog_vars)
    return forecast.predicted_mean

def forecast_var_model(model, test_data):
    forecast = model.forecast(test_data.values, steps=len(test_data))
    return forecast[:, 4]

def evaluate_rmse(actual, forecast):
    mask = ~np.isnan(actual) & (actual != 0)
    actual_filtered = actual[mask]
    forecast_filtered = forecast[mask]
    return mean_squared_error(actual_filtered, forecast_filtered, squared=False)

def evaluate_mape(actual, forecast):
    mask = ~np.isnan(actual) & (actual != 0)
    actual_filtered = actual[mask]
    forecast_filtered = forecast[mask]
    if len(actual_filtered) == 0:
        return np.nan
    return (np.abs((actual_filtered - forecast_filtered) / actual_filtered).mean()) * 100

def evaluate_accuracy(actual, forecast):
    mape = evaluate_mape(actual, forecast)
    if np.isnan(mape):
        return np.nan
    return 100 - mape

def validate_models(ticker, split_date, sarima_order=(1, 1, 1), sarima_seasonal_order=(1, 1, 1, 7), sarimax_order=(1, 1, 1), sarimax_seasonal_order=(1, 1, 1, 7), var_lag_order=8):
    try:
        df = fetch_data_from_mongodb(ticker)
        ts = preprocess_data(df)
        train, test = split_train_test(ts, split_date)
        
        # Train and forecast SARIMA model
        sarima_model = train_sarima_model(train, sarima_order, sarima_seasonal_order)
        sarima_forecast = forecast_sarima_model(sarima_model, test)
        
        # Train and forecast SARIMAX model
        sarimax_model = train_sarimax_model(train, sarimax_order, sarimax_seasonal_order)
        sarimax_forecast = forecast_sarimax_model(sarimax_model, test)
        
        # Train and forecast VAR model
        var_model = train_var_model(train, var_lag_order)
        var_forecast = forecast_var_model(var_model, test)

        sarima_forecast.index = test.index
        sarimax_forecast.index = test.index
        var_forecast = pd.Series(var_forecast, index=test.index)

        # Forecast VAR model
        var_forecast = forecast_var_model(var_model, test)

        # Print actual and predicted values for the test set
        print(f"Actual and Predicted Values for {ticker}:")
        print("Date\t\tActual\t\tSARIMA\t\tSARIMAX\t\tVAR")
        for idx in range(len(test)):
            date = test.index[idx]
            actual_value = test.iloc[idx]['Adj Close']
            sarima_pred = sarima_forecast[idx]
            sarimax_pred = sarimax_forecast[idx]
            var_pred = var_forecast[idx]
            print(f"{date}\t{actual_value:.2f}\t{sarima_pred:.2f}\t{sarimax_pred:.2f}\t{var_pred:.2f}")
        
        print()

        # Evaluate SARIMA model
        sarima_rmse = evaluate_rmse(test['Adj Close'], sarima_forecast)
        sarima_mape = evaluate_mape(test['Adj Close'], sarima_forecast)
        sarima_accuracy = evaluate_accuracy(test['Adj Close'], sarima_forecast)
        
        # Evaluate SARIMAX model
        sarimax_rmse = evaluate_rmse(test['Adj Close'], sarimax_forecast)
        sarimax_mape = evaluate_mape(test['Adj Close'], sarimax_forecast)
        sarimax_accuracy = evaluate_accuracy(test['Adj Close'], sarimax_forecast)
        
        # Evaluate VAR model
        var_rmse = evaluate_rmse(test['Adj Close'], var_forecast)
        var_mape = evaluate_mape(test['Adj Close'], var_forecast)
        var_accuracy = evaluate_accuracy(test['Adj Close'], var_forecast)


        
        print(f"Validation results for {ticker}:")
        print(f"SARIMA RMSE: {sarima_rmse}")
        print(f"SARIMA MAPE: {sarima_mape}")
        print(f"SARIMA Accuracy: {sarima_accuracy}%")
        print(f"SARIMAX RMSE: {sarimax_rmse}")
        print(f"SARIMAX MAPE: {sarimax_mape}")
        print(f"SARIMAX Accuracy: {sarimax_accuracy}%")
        print(f"VAR RMSE: {var_rmse}")
        print(f"VAR MAPE: {var_mape}")
        print(f"VAR Accuracy: {var_accuracy}%")
        
        return sarima_rmse, sarimax_rmse, var_rmse, sarima_accuracy, sarimax_accuracy, var_accuracy
    
    except Exception as e:
        print(f"Error validating models for {ticker}: {str(e)}")
        return None, None, None, None, None, None

if __name__ == '__main__':
    tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN','META','F','CAT','TCS.NS','WFC','TATASTEEL.NS','NFLX','RS','JPM','TSLA']
    split_date = datetime(2024, 6,1)
    
    for ticker in tickers:
        print(f"Validating models for {ticker}...")
        validate_models(ticker, split_date)
        print()
    
    # Close MongoDB client connection at the end
    client.close()











train.py
import pandas as pd
from pymongo import MongoClient
from statsmodels.tsa.statespace.sarimax import SARIMAX
import pickle

# Function to fetch historical data from MongoDB
def fetch_historical_data(ticker):
    try:
        client = MongoClient('mongodb://localhost:27017/')
        db = client['stockdata']
        collection = db['daily_price']
        cursor = collection.find({'Ticker': ticker})
        df = pd.DataFrame(list(cursor))
        client.close()
        return df
    except Exception as e:
        print(f"Error fetching data from MongoDB for {ticker}: {str(e)}")
        return None

# Function to preprocess data for SARIMAX model training
def preprocess_data(df):
    try:
        df['Date'] = pd.to_datetime(df['Date'])
        df.set_index('Date', inplace=True)
        df.sort_index(inplace=True)
        return df[['Adj Close']]  # Only 'Adj Close' is needed for training SARIMAX
    except Exception as e:
        print(f"Error preprocessing data for SARIMAX model: {str(e)}")
        return None

# Function to train SARIMAX model
def train_sarimax_model(train_data, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12)):
    try:
        sarimax_model = SARIMAX(train_data['Adj Close'], order=order, seasonal_order=seasonal_order, enforce_stationarity=False)
        sarimax_fit = sarimax_model.fit(disp=False)
        return sarimax_fit
    except Exception as e:
        print(f"Error training SARIMAX model: {str(e)}")
        return None

# Function to save trained SARIMAX model as a pickle file
def save_sarimax_model(model, ticker):
    try:
        with open(f'models/{ticker}_sarimax_model.pkl', 'wb') as f:
            pickle.dump(model, f)
        print(f"Saved SARIMAX model for {ticker}.")
    except Exception as e:
        print(f"Error saving SARIMAX model for {ticker}: {str(e)}")

if __name__ == '__main__':
    tickers = ['AAPL', 'MSFT']  # Add more tickers as needed
    
    for ticker in tickers:
        print(f"Training SARIMAX model for {ticker}...")
        data = fetch_historical_data(ticker)
        if data is None or data.empty:
            print(f"No data found for {ticker}. Skipping...")
            continue
        
        processed_data = preprocess_data(data)
        if processed_data is None:
            print(f"Error preprocessing data for {ticker}. Skipping...")
            continue
        
        sarimax_model = train_sarimax_model(processed_data)
        if sarimax_model is not None:
            save_sarimax_model(sarimax_model, ticker)
        else:
            print(f"Failed to train SARIMAX model for {ticker}. Skipping...")
        
        print()

    print("SARIMAX model training completed.")

app.py
from flask import Flask, request, jsonify
import pandas as pd
import pickle
from pymongo import MongoClient
from datetime import datetime, timedelta
from statsmodels.tsa.statespace.sarimax import SARIMAX
import warnings

app = Flask(__name__)

# Suppress warnings
warnings.filterwarnings("ignore")

# Function to load SARIMAX models from pickle files
def load_models():
    tickers = ['AAPL', 'MSFT']  # Add more tickers as needed
    models = {}
    for ticker in tickers:
        try:
            with open(f'models/{ticker}_sarimax_model.pkl', 'rb') as f:
                model = pickle.load(f)
                models[ticker] = model
                print(f"Loaded SARIMAX model for {ticker}.")
        except Exception as e:
            print(f"Error loading SARIMAX model for {ticker}: {str(e)}")
    return models

# Function to fetch historical data from MongoDB
def fetch_historical_data(ticker):
    try:
        client = MongoClient('mongodb://localhost:27017/')
        db = client['stockdata']
        collection = db['daily_price']
        cursor = collection.find({'Ticker': ticker})
        df = pd.DataFrame(list(cursor))
        client.close()
        return df
    except Exception as e:
        print(f"Error fetching historical data for {ticker}: {str(e)}")
        return None

# Function to preprocess data for SARIMAX model prediction
def preprocess_data_for_sarimax(df):
    try:
        df['Date'] = pd.to_datetime(df['Date'])
        df.set_index('Date', inplace=True)
        df.sort_index(inplace=True)
        return df[['Open', 'High', 'Low', 'Close', 'Volume']]
    except Exception as e:
        print(f"Error preprocessing data for SARIMAX model: {str(e)}")
        return None

# Function to predict stock prices using SARIMAX model
def predict_adj_close_prices(model, latest_data, days_ahead):
    try:
        ts = preprocess_data_for_sarimax(latest_data)
        if ts is None:
            print("Preprocessed data is None.")
            return None
        
        # Ensure exog_vars aligns with the forecast length
        exog_vars = ts[-days_ahead:]

        # Forecast using SARIMAX with exogenous variables
        forecast = model.get_forecast(steps=days_ahead, exog=exog_vars)
        predicted_adj_close = forecast.predicted_mean.values

        today = datetime.now().date()
        target_dates = [(today + timedelta(days=i)).strftime('%Y-%m-%d') for i in range(1, days_ahead + 1)]
        predictions = {target_dates[i]: predicted_adj_close[i] for i in range(days_ahead)}
        return predictions
    
    except Exception as e:
        print(f"Error predicting stock prices using SARIMAX model: {str(e)}")
        return None

# Load SARIMAX models on application startup
models = load_models()

@app.route('/predict/next_month', methods=['POST'])
def predict_next_month():
    ticker = request.json['ticker']
    
    try:
        if ticker not in models:
            return jsonify({'error': f'Model for {ticker} not found.'}), 404
        
        # Fetch historical data
        historical_data = fetch_historical_data(ticker)
        if historical_data is None or historical_data.empty:
            return jsonify({'error': f'Failed to fetch historical data for {ticker}.'}), 500
        
        # Predict next month's Adj Close prices using SARIMAX model
        predictions = predict_adj_close_prices(models[ticker], historical_data, days_ahead=30)
        
        if predictions is not None:
            return jsonify({'ticker': ticker, 'predictions': predictions})
        else:
            return jsonify({'error': 'Failed to predict stock prices.'}), 500
    
    except KeyError as ke:
        return jsonify({'error': f'Missing or incorrect parameter: {str(ke)}'}), 400
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    app.run(debug=True)

Give me the code to choose best order and seasonal order to produce accurate results









import pandas as pd
from datetime import datetime
from pymongo import MongoClient
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.vector_ar.var_model import VAR
from sklearn.metrics import mean_squared_error
import numpy as np
import os
import warnings
# Suppress warnings
warnings.filterwarnings("ignore")

# MongoDB connection setup
client = MongoClient('mongodb://localhost:27017/')
db = client['stockdata']
collection = db['daily_price']

def fetch_data_from_mongodb(ticker):
    cursor = collection.find({'Ticker': ticker})
    df = pd.DataFrame(list(cursor))
    return df

def preprocess_data(df):
    df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')
    df.set_index('Date', inplace=True)
    df.sort_index(inplace=True)
    df.index = pd.DatetimeIndex(df.index.values, freq=df.index.inferred_freq)
    return df[['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']]

def split_train_test(data, split_date):
    split_date = pd.to_datetime(split_date)
    train = data[data.index <= split_date]
    test = data[data.index > split_date]
    return train, test

def train_sarimax_model(train_data, order=(1, 1, 1), seasonal_order=(1, 1, 1, 7)):
    exog_vars = train_data[['Open', 'High', 'Low', 'Adj Close', 'Volume']]
    sarimax_model = SARIMAX(train_data['Adj Close'], exog=exog_vars, order=order, seasonal_order=seasonal_order, enforce_stationarity=False)
    sarimax_fit = sarimax_model.fit(disp=False)
    return sarimax_fit

def train_var_model(train_data, lag_order=8):
    var_model = VAR(train_data)
    var_fit = var_model.fit(lag_order)
    return var_fit

def forecast_sarimax_model(model, test_data):
    exog_vars = test_data[['Open', 'High', 'Low', 'Adj Close', 'Volume']]
    forecast = model.get_forecast(steps=len(test_data), exog=exog_vars)
    return forecast.predicted_mean

def forecast_var_model(model, test_data):
    forecast = model.forecast(test_data.values, steps=len(test_data))
    return forecast[:, 4]

def evaluate_rmse(actual, forecast):
    mask = ~np.isnan(actual) & (actual != 0)
    actual_filtered = actual[mask]
    forecast_filtered = forecast[mask]
    return mean_squared_error(actual_filtered, forecast_filtered, squared=False)

def evaluate_mape(actual, forecast):
    mask = ~np.isnan(actual) & (actual != 0)
    actual_filtered = actual[mask]
    forecast_filtered = forecast[mask]
    if len(actual_filtered) == 0:
        return np.nan
    return (np.abs((actual_filtered - forecast_filtered) / actual_filtered).mean()) * 100

def evaluate_accuracy(actual, forecast):
    mape = evaluate_mape(actual, forecast)
    if np.isnan(mape):
        return np.nan
    return 100 - mape

def validate_models(ticker, split_date, sarima_order=(1, 1, 1), sarima_seasonal_order=(1, 1, 1, 7), var_lag_order=7):
    try:
        df = fetch_data_from_mongodb(ticker)
        ts = preprocess_data(df)
        train, test = split_train_test(ts, split_date)
        
        # Train and forecast SARIMAX model
        sarimax_model = train_sarimax_model(train, sarima_order, sarima_seasonal_order)
        sarimax_forecast = forecast_sarimax_model(sarimax_model, test)
        
        # Train and forecast VAR model
        var_model = train_var_model(train, var_lag_order)
        var_forecast = forecast_var_model(var_model, test)

        sarimax_forecast.index = test.index
        var_forecast = pd.Series(var_forecast, index=test.index)

        # Evaluate SARIMAX model
        sarimax_rmse = evaluate_rmse(test['Adj Close'], sarimax_forecast)
        sarimax_mape = evaluate_mape(test['Adj Close'], sarimax_forecast)
        sarimax_accuracy = evaluate_accuracy(test['Adj Close'], sarimax_forecast)
        
        # Evaluate VAR model
        var_rmse = evaluate_rmse(test['Adj Close'], var_forecast)
        var_mape = evaluate_mape(test['Adj Close'], var_forecast)
        var_accuracy = evaluate_accuracy(test['Adj Close'], var_forecast)
        
        print(f"Validation results for {ticker}:")
        print(f"SARIMAX RMSE: {sarimax_rmse}")
        print(f"SARIMAX MAPE: {sarimax_mape}")
        print(f"SARIMAX Accuracy: {sarimax_accuracy}%")
        print(f"VAR RMSE: {var_rmse}")
        print(f"VAR MAPE: {var_mape}")
        print(f"VAR Accuracy: {var_accuracy}%")
        
        return sarimax_rmse, var_rmse, sarimax_accuracy, var_accuracy
    
    except Exception as e:
        print(f"Error validating models for {ticker}: {str(e)}")
        return None, None, None, None

if __name__ == '__main__':
    tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN','META','F','CAT','TCS.NS','WFC','TATASTEEL.NS','NFLX','RS','JPM','TSLA']
    split_date = datetime(2024, 2, 1)
    
    for ticker in tickers:
        print(f"Validating models for {ticker}...")
        validate_models(ticker, split_date)
        print()
    
    # Close MongoDB client connection at the end
    client.close()












import pandas as pd
from pymongo import MongoClient
from statsmodels.tsa.statespace.sarimax import SARIMAX
import pickle

# Function to fetch historical data from MongoDB
def fetch_historical_data(ticker):
    try:
        client = MongoClient('mongodb://localhost:27017/')
        db = client['stockdata']
        collection = db['daily_price']
        cursor = collection.find({'Ticker': ticker})
        df = pd.DataFrame(list(cursor))
        client.close()
        return df
    except Exception as e:
        print(f"Error fetching data from MongoDB for {ticker}: {str(e)}")
        return None

# Function to preprocess data for SARIMAX model training
def preprocess_data(df):
    try:
        df['Date'] = pd.to_datetime(df['Date'])
        df.set_index('Date', inplace=True)
        df.sort_index(inplace=True)
        return df[['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']]
    except Exception as e:
        print(f"Error preprocessing data for SARIMAX model: {str(e)}")
        return None

# Function to train SARIMAX model
def train_sarimax_model(train_data, order=(1, 1, 1), seasonal_order=(1, 1, 1, 7)):
    try:
        exog_vars = train_data[['Open', 'High', 'Low', 'Adj Close', 'Volume']]
        sarimax_model = SARIMAX(train_data['Adj Close'], exog=exog_vars, order=order, seasonal_order=seasonal_order, enforce_stationarity=False)
        sarimax_fit = sarimax_model.fit(disp=False)
        return sarimax_fit
    except Exception as e:
        print(f"Error training SARIMAX model: {str(e)}")
        return None

# Function to save trained SARIMAX model as a pickle file
def save_sarimax_model(model, ticker):
    try:
        with open(f'models/{ticker}_sarimax_model.pkl', 'wb') as f:
            pickle.dump(model, f)
        print(f"Saved SARIMAX model for {ticker}.")
    except Exception as e:
        print(f"Error saving SARIMAX model for {ticker}: {str(e)}")

if __name__ == '__main__':
    tickers = ['AAPL', 'MSFT']
    
    for ticker in tickers:
        print(f"Training SARIMAX model for {ticker}...")
        data = fetch_historical_data(ticker)
        if data is None or data.empty:
            print(f"No data found for {ticker}. Skipping...")
            continue
        
        processed_data = preprocess_data(data)
        if processed_data is None:
            print(f"Error preprocessing data for {ticker}. Skipping...")
            continue
        
        sarimax_model = train_sarimax_model(processed_data)
        if sarimax_model is not None:
            save_sarimax_model(sarimax_model, ticker)
        else:
            print(f"Failed to train SARIMAX model for {ticker}. Skipping...")
        
        print()

    print("SARIMAX model training completed.")






from flask import Flask, request, jsonify
import pandas as pd
import pickle
from pymongo import MongoClient
from datetime import datetime, timedelta
from statsmodels.tsa.statespace.sarimax import SARIMAX
import warnings

app = Flask(__name__)

# Suppress warnings
warnings.filterwarnings("ignore")

# Function to load SARIMAX models from pickle files
def load_models():
    tickers = ['AAPL', 'MSFT']  # Add more tickers as needed
    models = {}
    for ticker in tickers:
        try:
            with open(f'models/{ticker}_sarimax_model.pkl', 'rb') as f:
                model = pickle.load(f)
                models[ticker] = model
                print(f"Loaded SARIMAX model for {ticker}.")
        except Exception as e:
            print(f"Error loading SARIMAX model for {ticker}: {str(e)}")
    return models

# Function to fetch historical data from MongoDB
def fetch_historical_data(ticker):
    try:
        client = MongoClient('mongodb://localhost:27017/')
        db = client['stockdata']
        collection = db['daily_price']
        cursor = collection.find({'Ticker': ticker})
        df = pd.DataFrame(list(cursor))
        client.close()
        return df
    except Exception as e:
        print(f"Error fetching historical data for {ticker}: {str(e)}")
        return None

# Function to preprocess data for SARIMAX model prediction
def preprocess_data_for_sarimax(df):
    try:
        df['Date'] = pd.to_datetime(df['Date'])
        df.set_index('Date', inplace=True)
        df.sort_index(inplace=True)
        return df[['Open', 'High', 'Low', 'Close', 'Volume']]
    except Exception as e:
        print(f"Error preprocessing data for SARIMAX model: {str(e)}")
        return None

# Function to predict stock prices using SARIMAX model
def predict_stock_prices(model, latest_data, days_ahead):
    try:
        ts = preprocess_data_for_sarimax(latest_data)
        if ts is None:
            print("Preprocessed data is None.")
            return None

        # Select exogenous variables ('Open', 'High', 'Low', 'Close', 'Volume')
        exog_vars = ts[['Open', 'High', 'Low', 'Close', 'Volume']]
        
        # Ensure exog_vars aligns with the forecast length
        exog_vars = exog_vars[-days_ahead:]

        # Forecast using SARIMAX with exogenous variables
        forecast = model.get_forecast(steps=days_ahead, exog=exog_vars)
        predicted_closes = forecast.predicted_mean.values

        today = datetime.now().date()
        target_dates = [(today + timedelta(days=i)).strftime('%Y-%m-%d') for i in range(1, days_ahead + 1)]
        predictions = {target_dates[i]: predicted_closes[i] for i in range(days_ahead)}
        return predictions
    
    except Exception as e:
        print(f"Error predicting stock prices using SARIMAX model: {str(e)}")
        return None

# Load SARIMAX models on application startup
models = load_models()

@app.route('/predict/next_month', methods=['POST'])
def predict_next_month():
    ticker = request.json['ticker']
    
    try:
        if ticker not in models:
            return jsonify({'error': f'Model for {ticker} not found.'}), 404
        
        # Fetch historical data
        historical_data = fetch_historical_data(ticker)
        if historical_data is None or historical_data.empty:
            return jsonify({'error': f'Failed to fetch historical data for {ticker}.'}), 500
        
        # Predict next month's Adj Close prices using SARIMAX model
        predictions = predict_stock_prices(models[ticker], historical_data, days_ahead=30)
        
        if predictions is not None:
            return jsonify({'ticker': ticker, 'predictions': predictions})
        else:
            return jsonify({'error': 'Failed to predict stock prices.'}), 500
    
    except KeyError as ke:
        return jsonify({'error': f'Missing or incorrect parameter: {str(ke)}'}), 400
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    app.run(debug=True)
